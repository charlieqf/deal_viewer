# -*- coding: utf-8 -*-
import smtplib
from email.mime.text import MIMEText
import os
import re
from pypinyin import lazy_pinyin
import pandas as pd
import pymssql
import requests
import json
import time
import random
import ftplib
from datetime import datetime
import pyodbc
import chardet
import threading
import io
from dateutil.parser import parse
import socket
from urllib.parse import quote


socket.setdefaulttimeout(600)

# ==================== DEBUG TRACKING REMOVED ====================

# FTP server details
FTP_HOST = "113.125.202.171"
FTP_PORT = 11421
FTP_USER = "dv"
FTP_PASS = "246qweASD@"
FTP_HOME_DIR = "DataTeam"
FTP_HOME_DIR = "/"

# FTP2 server details 192.168.1.211
FTP2_HOST = "113.125.202.171"
FTP2_PORT = 21121
FTP2_USER = "gsuser"
FTP2_PASS = "Password01"
FTP2_HOME_DIR = "."


UPDATE_LOG_PATH = "/Products_log/æ›´æ–°æ—¶é—´TXTè®°å½•/ABNæ›´æ–°æ—¶é—´sms.txt"
UPDATE_LOG_PATH_YYBG = "/Products_log/æ›´æ–°æ—¶é—´TXTè®°å½•/ABNæ›´æ–°æ—¶é—´sms_yybg.txt"
FTP_FOLDER_PATH = "/Products_log/é“¶è¡Œé—´å€ºåˆ¸å¸‚åœºæ›´æ–°æ•°æ®"
INCREMENT_FOLDER_PATH = "/å¢é‡æ–‡æ¡£"
DV_FOLDER_PATH = "/DealViewer/TrustAssociatedDoc"
DV_CREDIT_RATING_FOLDER = "ProductCreditRatingFiles"
DV_RELEASE_INSTRUCTION_FOLDER = "ProductReleaseInstructions"
DV_TRUSTEE_REPORT_FOLDER = "TrusteeReport"

current_dir = os.path.dirname(os.path.abspath(__file__))
cache_folder = os.path.join(current_dir, "abn_file_cache")
if not os.path.exists(cache_folder):
    os.makedirs(cache_folder)


def list_ftp_directory_with_retry(ftp, path, retries=6):
    for attempt in range(retries):
        try:
            return list_ftp_directory(ftp, path)
        except (
            ftplib.error_temp,
            ftplib.error_perm,
            ftplib.error_reply,
            BrokenPipeError,
            TimeoutError,
            ConnectionError,
            socket.timeout,
            EOFError,
        ) as e:
            print(
                f"Error occurred while listing directory: {e}. Retrying {retries - attempt - 1} more times."
            )
            # å¦‚æœæ˜¯è¶…æ—¶é”™è¯¯ï¼Œå¯èƒ½éœ€è¦é‡æ–°è¿æ¥FTP
            if (
                isinstance(e, (TimeoutError, socket.timeout, ConnectionError, EOFError))
                and attempt < retries - 1
            ):
                try:
                    print("Connection timed out, attempting to reconnect FTP...")
                    # ä¿å­˜åŸå§‹è¿æ¥ä¿¡æ¯
                    host = ftp.host
                    port = ftp.port
                    user = (
                        ftp._user if hasattr(ftp, "_user") else FTP_USER
                    )  # å¤‡ç”¨ç”¨æˆ·å
                    passwd = (
                        ftp._passwd if hasattr(ftp, "_passwd") else FTP_PASS
                    )  # å¤‡ç”¨å¯†ç 

                    # é‡æ–°è¿æ¥
                    try:
                        ftp.close()
                    except:
                        pass  # å¿½ç•¥å…³é—­è¿æ¥æ—¶çš„é”™è¯¯

                    ftp.connect(host, port, timeout=60)  # å¢åŠ è¶…æ—¶æ—¶é—´
                    ftp.login(user, passwd)
                    print("FTP reconnection successful")
                except Exception as reconnect_error:
                    print(f"Failed to reconnect to FTP: {reconnect_error}")

            # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œé¿å…ç«‹å³é‡è¯•å¯¼è‡´å†æ¬¡è¶…æ—¶
            backoff_time = 5 * (attempt + 1)  # é€æ¸å¢åŠ ç­‰å¾…æ—¶é—´
            print(f"Waiting {backoff_time} seconds before retrying...")
            time.sleep(backoff_time)

    raise Exception("Failed to list directory after multiple attempts")


def list_ftp_directory(ftp, path):
    """List files and directories in the given FTP path."""
    print(f"Listing directory: {path}")

    try:
        # åˆ‡æ¢ç›®å½•
        ftp.cwd(path)
    except ftplib.error_perm as e:
        if "550" in str(e):
            print(f"Directory {path} does not exist.")
            return []
        else:
            raise

    raw_items = []

    try:
        # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        if hasattr(ftp, "sock") and ftp.sock:
            ftp.sock.settimeout(120)  # 2åˆ†é’Ÿ

        # è·å–ç›®å½•åˆ—è¡¨
        ftp.retrlines("NLST", raw_items.append)
    except ftplib.error_perm as e:
        print(f"NLST command failed: {e}, trying LIST instead")
        try:
            # å¦‚æœNLSTæŒ‡ä»¤å¤±è´¥ï¼Œå°è¯•LIST
            temp_items = []
            ftp.retrlines("LIST", temp_items.append)
            # ä»LISTç»“æœä¸­æå–æ–‡ä»¶åï¼ˆæœ€åä¸€åˆ—ï¼‰
            raw_items = [item.split()[-1] for item in temp_items if item.strip()]
        except Exception as list_error:
            print(f"LIST command also failed: {list_error}")
            # Propagate error if it's a connection issue (e.g. timeout) to let retry logic work?
            # If LIST failed, we probably want to retry.
            # But here we are inside 'except ftplib.error_perm'. error_perm is usually permanent?
            pass
    except Exception as e:
        print(f"Error listing directory: {e}")
        # CRITICAL: Re-raise the exception so retry logic can handle it!
        raise
    finally:
        # æ¢å¤åŸå§‹è¶…æ—¶è®¾ç½®
        if hasattr(ftp, "sock") and ftp.sock:
            ftp.sock.settimeout(None)

    # è¿‡æ»¤ç©ºé¡¹å¹¶è¿”å›
    items = [item for item in raw_items if item.strip()]

    print(f"Found {len(items)} items in {path}")
    return items


def read_ftp_file(ftp, file_path):
    with io.BytesIO() as bio:
        ftp.retrbinary(f"RETR {file_path}", bio.write)
        bio.seek(0)
        return bio.read().decode("utf-8")


def enable_utf8(ftp):
    response = ftp.sendcmd("OPTS UTF8 ON")
    if "200" in response:
        print("UTF-8 encoding enabled on the FTP server.")
    else:
        print("Failed to enable UTF-8 encoding on the FTP server.")


def ftp1_keep_alive(interval=30, retries=6):
    while True:
        reconnect = False
        remaining_retries = retries
        while remaining_retries > 0:
            try:
                ftp.voidcmd("NOOP")
                reconnect = False
            except ftplib.all_errors as e:
                print(f"ftp1 Keep-alive error: {e}")
                reconnect = True
            time.sleep(interval)  # Keep-alive interval
            remaining_retries -= 1

        if reconnect:
            ftp.close()
            ftp.connect(FTP_HOST, FTP_PORT, timeout=600)
            ftp.login(FTP_USER, FTP_PASS)


def ftp2_keep_alive(interval=30, retries=6):
    while True:
        reconnect = False
        remaining_retries = retries
        while remaining_retries > 0:
            try:
                ftp2.voidcmd("NOOP")
                reconnect = False
            except ftplib.all_errors as e:
                print(f"ftp2 Keep-alive error: {e}")
                reconnect = True
            time.sleep(interval)  # Keep-alive interval
            remaining_retries -= 1

        if reconnect:
            ftp2.close()
            ftp2.connect(FTP2_HOST, FTP2_PORT, timeout=600)
            ftp2.login(FTP2_USER, FTP2_PASS)


def upload_file_to_ftp(ftp, local_file_path, ftp_folder, ftp_file_path, file_name):
    # if file does not exist on FTP, upload the file
    if file_name not in list_ftp_directory_with_retry(ftp, ftp_folder):
        print("Writing PDF to", ftp_file_path, "=====>")
        with open(local_file_path, "rb") as f:
            ftp.storbinary(f"STOR {ftp_file_path}", f)
    else:
        print("File already exists on FTP in folder:", ftp_folder)


# Connect to FTP server 10.0.0.114
ftp = ftplib.FTP()
ftp.connect(FTP_HOST, FTP_PORT, timeout=600)
ftp.login(FTP_USER, FTP_PASS)
ftp.set_pasv(True)
# ftp.set_debuglevel(2)
# ftp.cwd(FTP_HOME_DIR)


# Connect to FTP server 192.168.1.211
ftp2 = ftplib.FTP()
ftp2.connect(FTP2_HOST, FTP2_PORT, timeout=600)
ftp2.login(FTP2_USER, FTP2_PASS)
ftp2.set_pasv(True)
# ftp2.set_debuglevel(2)
enable_utf8(ftp2)
ftp2.encoding = "utf-8"

# Start a thread to keep the connection alive
keep_alive_thread = threading.Thread(
    target=ftp1_keep_alive, args=(30, 3)
)  # NOOP every 5 minutes
keep_alive_thread.daemon = True
keep_alive_thread.start()

keep_alive_thread2 = threading.Thread(
    target=ftp2_keep_alive, args=(30, 3)
)  # NOOP every 5 minutes
keep_alive_thread2.daemon = True
keep_alive_thread2.start()


def get_web_pdf_content_with_retry(web_pdf_path, retries=3):
    for attempt in range(retries):
        try:
            return get_web_pdf_content(web_pdf_path)
        except Exception as e:
            print(
                f"Error occurred while getting PDF content from {web_pdf_path}: {e}. Retrying {retries - attempt - 1} more times."
            )
            time.sleep(5)  # Wait for 5 seconds before retrying

    raise Exception("Failed to get PDF content after multiple attempts")


def get_web_pdf_content(web_pdf_path):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36",
        "Accept": "application/pdf",
    }

    # First try without using SmartProxy
    print("Attempting to download without proxy first...")
    try:
        response = requests.get(web_pdf_path, headers=headers, timeout=(10, 30))

        # Check if request was successful
        content_type = response.headers.get("Content-Type", "").lower()
        if (
            response.status_code == 200
            and ("application/pdf" in content_type or response.content.startswith(b"%PDF"))
        ):
            print("Download successful without proxy")
            return True, response.content

        # Check if this is an IP-related failure
        ip_related_failure = False
        if response.status_code in [403, 429, 451]:
            ip_related_failure = True
        elif response.status_code != 200:
            # Check response text for IP blocking messages
            ip_block_indicators = [
                "blocked",
                "forbidden",
                "access denied",
                "IP",
                "åœ°å€è¢«ç¦æ­¢",
                "è®¿é—®å—é™",
                "è®¿é—®è¢«æ‹’ç»",
            ]
            response_text = response.text.lower()
            for indicator in ip_block_indicators:
                if indicator.lower() in response_text:
                    ip_related_failure = True
                    break

        if not ip_related_failure:
            # Failed but not due to IP issues
            print(
                f"Failed to download without proxy. Status: {response.status_code}. Not IP related."
            )
            return False, response.text[:200]  # Truncate error message
    except Exception as e:
        print(f"Error during non-proxy download attempt: {e}")
        # For connection errors, assume it could be IP related
        if isinstance(
            e, (requests.exceptions.ConnectionError, requests.exceptions.Timeout)
        ):
            print("Connection error - might be IP related")
        else:
            # For other types of errors, return the error
            return False, str(e)

    # If we got here, either there was an IP-related failure or an exception occurred
    # Try again with SmartProxy (only if proxy is configured)
    if USE_PROXY and proxies is not None:
        print("Trying download with SmartProxy...")
        try:
            response = requests.get(
                web_pdf_path, headers=headers, proxies=proxies, timeout=(10, 30)
            )

            # Check if the content is actually a PDF
            print("Checking content from proxy request")
            # Loose check: Content-Type OR Magic Number
            content_type = response.headers.get("Content-Type", "").lower()
            if (
                "application/pdf" in content_type
                or response.content.startswith(b"%PDF")
            ):
                print("Successfully downloaded with proxy")
                return True, response.content
            else:
                print(f"Failed to download with proxy. Status: {response.status_code}")
                return False, response.text[:200]  # Truncate error message
        except Exception as e:
            print(f"Error during proxy download attempt: {e}")
            return False, str(e)
    else:
        print("IP restriction detected but no proxy configured. Cannot download file.")
        return False, "No proxy available for IP-restricted download"


def create_dir_on_ftp(ftp, dir, folder):
    # check if the folder exists on the ftp server, if not, create the folder
    folder_path = os.path.join(dir, folder)
    if folder not in list_ftp_directory_with_retry(ftp, dir):
        print("create new folder: ", folder_path)
        try:
            ftp.mkd(folder_path)
        except ftplib.error_perm as e:
            # If directory already exists (550), ignore it
            if "550" in str(e):
                print(f"Folder {folder_path} already exists (caught 550)")
            else:
                raise
    else:
        print(folder, "already exists in", dir, "on FTP")

    return folder_path


def get_sql_connection():
    try:
        # Establish connection using pyodbc
        conn_str = (
            "Driver={ODBC Driver 18 for SQL Server};"
            "Server=113.125.202.171,52482;"
            "Database=PortfolioManagement;"  # Use your database name
            "UID=sa;"  # Use your username
            "PWD=PasswordGS2017;"  # Use your password
            "Encrypt=no;"
            "TrustServerCertificate=yes;"
        )
        conn = pyodbc.connect(conn_str)

        return conn
    except pyodbc.Error as e:
        print("Database error:", e)


conn = get_sql_connection()
print("got connection.")

# # Smartproxy credentials and proxy URL
# username = "splci64blr"
# password = "6j0z1hrwFM4LbdheZ_"
# proxy_url = f"http://{username}:{password}@gate.smartproxy.com:10001"

# ä»£ç†é…ç½®
# ProxyJetè¿”å›423 Lockedé”™è¯¯ï¼ˆè´¦æˆ·é”å®š/é™é¢/æœåŠ¡ä¸å¯ç”¨ï¼‰
# ç”±äºè„šæœ¬åœ¨ä¸­å›½å¢ƒå†…è¿è¡Œï¼Œä¸”ç›´æ¥è®¿é—®å¯ä»¥è·å–tokenï¼Œå°è¯•ä¸ä½¿ç”¨ä»£ç†

# é€‰é¡¹1ï¼šä¸ä½¿ç”¨ä»£ç†ï¼ˆå½“å‰é‡åˆ°IPé™åˆ¶ï¼Œéœ€è¦ä½¿ç”¨ä»£ç†ï¼‰
# USE_PROXY = False
# proxies = None
# proxy_url = "None (Direct Connection)"

# é€‰é¡¹2ï¼šä½¿ç”¨ProxyJetä»£ç†ï¼ˆå·²å¯ç”¨ä»¥ç»•è¿‡IPé™åˆ¶ï¼‰
USE_PROXY = True
proxy_string = "in.proxy-jet.io:1010:2506034iYZQ-resi_region-CN_Guangdong_Guangzhou-ip-7193938:rUGciFpmX7CwT12"
# å¤‡ç”¨åŒ—äº¬IPï¼š
# proxy_string = "in.proxy-jet.io:1010:2506034iYZQ-resi_region-CN_Beijing_Jinrongjie-ip-1059836:rUGciFpmX7CwT12"

parts = proxy_string.split(":")
hostname = parts[0]
port = parts[1]
username = parts[2]
password = parts[3]
proxy_url = f"http://{username}:{password}@{hostname}:{port}"
proxies = {
    "http": proxy_url,
    "https": proxy_url,
}

# conn = pymssql.connect(host='172.16.6.143\mssql', user='sa', password='PasswordGS2017',
#                        database='PortfolioManagement', charset='utf8')
# cur = conn.cursor()

# headers = {
#     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36",
#     "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
#     "Accept-Language": "en-US,en;q=0.9",
#     "Accept-Encoding": "gzip, deflate, br",
#     # Add a referer if necessary, for instance, the main page or the previous page you'd access in a typical flow
#     "Referer": "https://www.chinamoney.com.cn/"
# }
static_url = "https://www.chinamoney.com.cn/dqs/cm-s-notice-query/fileDownLoad.do?mode=open&contentId={}&priority=0"


def get_headers(url, use="pc"):
    pc_agent = [
        "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50",
        "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50",
        "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0);",
        "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
        "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",
        "Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",
        "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
        "Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36"
        "Mozilla/5.0 (X11; Linux x86_64; rv:76.0) Gecko/20100101 Firefox/76.0",
    ]
    phone_agent = [
        "Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5",
        "Mozilla/5.0 (iPod; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5",
        "Mozilla/5.0 (iPad; U; CPU OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5",
        "Mozilla/5.0 (Linux; U; Android 2.3.7; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
        "MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
        "Opera/9.80 (Android 2.3.4; Linux; Opera Mobi/build-1107180945; U; en-GB) Presto/2.8.149 Version/11.10",
        "Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/534.13 (KHTML, like Gecko) Version/4.0 Safari/534.13",
        "Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; en) AppleWebKit/534.1+ (KHTML, like Gecko) Version/6.0.0.337 Mobile Safari/534.1+",
        "Mozilla/5.0 (hp-tablet; Linux; hpwOS/3.0.0; U; en-US) AppleWebKit/534.6 (KHTML, like Gecko) wOSBrowser/233.70 Safari/534.6 TouchPad/1.0",
        "Mozilla/5.0 (SymbianOS/9.4; Series60/5.0 NokiaN97-1/20.0.019; Profile/MIDP-2.1 Configuration/CLDC-1.1) AppleWebKit/525 (KHTML, like Gecko) BrowserNG/7.1.18124",
        "Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0; HTC; Titan)",
        "UCWEB7.0.2.37/28/999",
        # "NOKIA5700/ UCWEB7.0.2.37/28/999",
        "Openwave/ UCWEB7.0.2.37/28/999",
        "Mozilla/4.0 (compatible; MSIE 6.0; ) Opera/UCWEB7.0.2.37/28/999",
    ]
    """user_agentéƒ¨åˆ†æ¥æº:https://blog.csdn.net/IT__LS/java/article/details/78880903"""
    referer = lambda url: re.search(
        "^((https://)|(https://))?([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,6}(/)",
        url,
    ).group()
    """æ­£åˆ™æ¥æº:https://www.cnblogs.com/blacksonny/p/6055357.html"""
    if use == "phone":  # éšæœºé€‰æ‹©ä¸€ä¸ª
        agent = random.choice(phone_agent)
    else:
        agent = random.choice(pc_agent)

    headers = {
        "User-Agent": agent,
        "Referer": "https://www.chinamoney.com.cn/chinese/qwjsn/",
        "Cookie": "_ulta_id.CM-Prod.e9dc=9f8733fe96d3a061; AlteonP10=CLDtOSw/F6xDU59z7jzUGg$$; apache=4a63b086221745dd13be58c2f7de0338; ags=2a1ba4d47b619c011c19c1cc4b3c0c32; lss=fd9e664ef34511dcdc4a51a4e8d84abc; _ulta_ses.CM-Prod.e9dc=1187f9d14693de99; isLogin=0",
    }
    return headers


def get_proxy():
    return requests.get("http://162.14.106.158:5010/get/?type=http").json()


def delete_proxy(proxy):
    requests.get("http://162.14.106.158:5010/delete/?proxy={}".format(proxy))


def get_sign_and_info_level():
    baseurl = "https://www.chinamoney.com.cn/lss/rest/cm-s-account/getLT"
    params = {"type": "0"}

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        "Content-Length": "0",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Referer": "https://www.chinamoney.com.cn/chinese/qwjsn/",
        "Cookie": "_ulta_id.CM-Prod.e9dc=9f8733fe96d3a061; AlteonP10=CLDtOSw/F6xDU59z7jzUGg$$; apache=4a63b086221745dd13be58c2f7de0338; ags=2a1ba4d47b619c011c19c1cc4b3c0c32; lss=fd9e664ef34511dcdc4a51a4e8d84abc; _ulta_ses.CM-Prod.e9dc=1187f9d14693de99; isLogin=0",
    }

    # headers = {
    #     "Accept": "*/*",
    #     "Accept-Encoding": "gzip, deflate, br, zstd",
    #     "Accept-Language": "en-GB,en-US;q=0.9,en;q=0.8",
    #     "Cache-Control": "no-cache",
    #     "Connection": "keep-alive",
    #     "Content-Length": "0",
    #     "Cookie": "apache=bbfde8c184f3e1c6074ffab28a313c87; ags=a23ede7e97bccb1b2380be21609ada80; lss=f7cb2cf4b1607aec30e411e90d47c685; _ulta_id.CM-Prod.e9dc=857b1fa51afb521c; AlteonP10=Avl2Wiw/F6xv9116mHKodw$$; isLogin=0; _ulta_ses.CM-Prod.e9dc=43636e15f932ef74",
    #     "Host": "www.chinamoney.com.cn",
    #     "Origin": "https://www.chinamoney.com.cn",
    #     "Pragma": "no-cache",
    #     "Referer": "https://www.chinamoney.com.cn/chinese/qwjsn/?searchValue=%25E4%25B8%258A%25E5%25B8%2582%25E6%25B5%2581%25E9%2580%259A%2520ABN",
    #     "Sec-Fetch-Dest": "empty",
    #     "Sec-Fetch-Mode": "cors",
    #     "Sec-Fetch-Site": "same-origin",
    #     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
    #     "X-Requested-With": "XMLHttpRequest",
    #     "sec-ch-ua": '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
    #     "sec-ch-ua-mobile": "?0",
    #     "sec-ch-ua-platform": '"Windows"'
    # }

    try:
        # First try without using proxy to save costs
        print("Attempting to get sign and info_level without proxy...")
        response = requests.post(baseurl, data=params, headers=headers)
        print(f"Response status: {response.status_code}")

        # Check if this is an IP-related failure
        ip_related_failure = False
        if response.status_code in [403, 429, 451]:
            ip_related_failure = True
            print(f"Received {response.status_code} error - likely IP restriction")
        elif response.status_code != 200:
            try:
                response_text = response.text.lower()
                ip_block_indicators = [
                    "blocked",
                    "forbidden",
                    "access denied",
                    "åœ°å€è¢«ç¦æ­¢",
                    "è®¿é—®å—é™",
                    "è®¿é—®è¢«æ‹’ç»",
                ]
                for indicator in ip_block_indicators:
                    if indicator.lower() in response_text:
                        ip_related_failure = True
                        print(f"Detected IP blocking indicator: {indicator}")
                        break
            except:
                pass

        # If IP-related failure, retry with proxy (only if proxy is configured)
        if ip_related_failure:
            if USE_PROXY and proxies is not None:
                print("Retrying with SmartProxy due to IP restriction...")
                response = requests.post(
                    baseurl, data=params, headers=headers, proxies=proxies
                )
                print(f"Response status with proxy: {response.status_code}")
            else:
                print("IP restriction detected but no proxy configured. Cannot retry.")
                print(f"Response text (first 500 chars): {response.text[:500]}")
                raise Exception(
                    f"IP restriction detected (HTTP {response.status_code}) and no proxy available"
                )

        # ä½¿ç”¨ä»£ç†è®¿é—®
        requestsdata = response.json()
        data = requestsdata["data"]
        print("data =", data)

        info_level = data["UT"].replace("\n", "")
        sign = data["sign"].replace("\n", "")

        print("sign =", sign)
        print("info_level =", info_level)

        return sign, info_level

    except Exception as e:
        print("error getting sign and info_level:", e)
        return None, None


# è·å–Signå’ŒInfoLevel
def get_token():
    baseurl = "https://www.chinamoney.com.cn/lss/rest/cm-s-account/getLT"
    params = {"type": "0"}

    # headers = {
    #     'User-Agent': 'Mozilla/5.0',
    #     'Content-Type': 'application/json',
    # }
    # # Define headers for the request
    # headers = {
    #     "Content-Type": "application/x-www-form-urlencoded",
    #     "Accept": "application/json, text/javascript, */*; q=0.01",
    #     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36",
    # }
    headers = get_headers("https://www.chinamoney.com.cn/chinese/qwjsn/?")

    try:
        # First try without using proxy to save costs
        print("Attempting to get token without proxy...")
        response = requests.post(baseurl, data=params, headers=headers)
        print(f"Response status: {response.status_code}")

        # Check if this is an IP-related failure
        ip_related_failure = False
        if response.status_code in [403, 429, 451]:
            ip_related_failure = True
            print(f"Received {response.status_code} error - likely IP restriction")
        elif response.status_code != 200:
            try:
                response_text = response.text.lower()
                ip_block_indicators = [
                    "blocked",
                    "forbidden",
                    "access denied",
                    "åœ°å€è¢«ç¦æ­¢",
                    "è®¿é—®å—é™",
                    "è®¿é—®è¢«æ‹’ç»",
                ]
                for indicator in ip_block_indicators:
                    if indicator.lower() in response_text:
                        ip_related_failure = True
                        print(f"Detected IP blocking indicator: {indicator}")
                        break
            except:
                pass

        # If IP-related failure, retry with proxy (only if proxy is configured)
        if ip_related_failure:
            if USE_PROXY and proxies is not None:
                print("Retrying with SmartProxy due to IP restriction...")
                response = requests.post(
                    baseurl, data=params, headers=headers, proxies=proxies
                )
                print(f"Response status with proxy: {response.status_code}")
            else:
                print("IP restriction detected but no proxy configured. Cannot retry.")
                print(f"Response text (first 500 chars): {response.text[:500]}")
                raise Exception(
                    f"IP restriction detected (HTTP {response.status_code}) and no proxy available"
                )

        # ä½¿ç”¨ä»£ç†è®¿é—®
        requestsdata = response.json()
        data = requestsdata["data"]
        print("data =", data)
        # æ‰“å°å“åº”å†…å®¹
        return data
    except Exception as e:
        print(f"Error getting token: {e}")
        return None


def get_html(timestamp):
    # Only test proxy if it's configured
    if USE_PROXY and proxies is not None:
        test_url = "https://ip.smartproxy.com/json"
        try:
            response = requests.get(test_url, proxies=proxies)
            print(response.status_code, response.json())
        except Exception as e:
            print(f"Proxy test failed: {e}")
    else:
        print("Proxy not configured, skipping proxy test...")

    # åŠ¨æ€è·å–tokenï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆæé«˜å®¹é”™æ€§
    print("è·å–æ–°çš„token...")
    token = get_token()
    if token is None:
        print("è­¦å‘Š: æ— æ³•è·å–tokenï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
        sign, info_level = get_sign_and_info_level()
        if sign is None or info_level is None:
            raise Exception("æ— æ³•è·å–æœ‰æ•ˆçš„tokenå’Œsignï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®")
    else:
        print("token is " + str(token))
        info_level = token["UT"].replace("\n", "")
        sign = token["sign"].replace("\n", "")
    
    print("info_level:", info_level[:50] + "...")
    print("sign:", sign[:50] + "...")

    # åˆ›å»ºSessionå¯¹è±¡æ¥è‡ªåŠ¨ç®¡ç†Cookie
    session = requests.Session()

    # å…ˆè®¿é—®ä¸»é¡µè·å–æœ‰æ•ˆçš„Cookie
    print("è®¿é—®ä¸»é¡µè·å–Cookie...")
    try:
        home_url = "https://www.chinamoney.com.cn/chinese/qwjsn/"
        home_headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Encoding": "gzip, deflate, br, zstd",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Host": "www.chinamoney.com.cn",
            "Pragma": "no-cache",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Upgrade-Insecure-Requests": "1",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
            "sec-ch-ua": '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
        }
        home_response = session.get(home_url, headers=home_headers, verify=False, timeout=10)
        print(f"ä¸»é¡µè®¿é—®çŠ¶æ€: {home_response.status_code}")
        print(f"è·å–åˆ°çš„Cookie: {session.cookies.get_dict()}")

        # å¦‚æœæ²¡æœ‰è‡ªåŠ¨è·å–åˆ°Cookieï¼Œæ‰‹åŠ¨è®¾ç½®
        if not session.cookies.get_dict():
            print("æœªè‡ªåŠ¨è·å–åˆ°Cookieï¼Œæ‰‹åŠ¨è®¾ç½®æµè§ˆå™¨Cookie...")
            session.cookies.set("AlteonP10", "AYasNCw/F6zhU0YxOaADcg$$", domain="www.chinamoney.com.cn")
            session.cookies.set("lss", "953d30744145d363215192a47c98ceb5", domain="www.chinamoney.com.cn")
            session.cookies.set("isLogin", "0", domain="www.chinamoney.com.cn")
            print(f"æ‰‹åŠ¨è®¾ç½®åçš„Cookie: {session.cookies.get_dict()}")
    except Exception as e:
        print(f"è®¿é—®ä¸»é¡µå¤±è´¥: {e}")
        print("ç»§ç»­å°è¯•APIè¯·æ±‚...")

    baseurl = "https://www.chinamoney.com.cn/ses/rest/cm-u-notice-ses-cn/query"

    # ==================== æ—¥æœŸèŒƒå›´é…ç½® ====================
    # ä½¿ç”¨æ—¥æœŸèŒƒå›´æŸ¥è¯¢ï¼Œä¸ä¾èµ–æ¸¸æ ‡åˆ†é¡µ
    USE_DATE_RANGE = False  # æ”¹ä¸ºFalseä½¿ç”¨æ—¶é—´æˆ³+æ¸¸æ ‡åˆ†é¡µ
    START_DATE = "2025-08-01"  # èµ·å§‹æ—¥æœŸï¼ˆUSE_DATE_RANGE=Trueæ—¶ç”Ÿæ•ˆï¼‰
    END_DATE = "2025-10-01"    # ç»“æŸæ—¥æœŸï¼ˆUSE_DATE_RANGE=Trueæ—¶ç”Ÿæ•ˆï¼‰
    # ====================================================
    
    params = {
        "sort": "date",
        "text": "ä¸Šå¸‚æµé€š ABN",
        "date": "customize" if USE_DATE_RANGE else "all",  # è‡ªå®šä¹‰æ—¥æœŸèŒƒå›´
        "field": "title",
        "start": START_DATE if USE_DATE_RANGE else "",
        "end": END_DATE if USE_DATE_RANGE else "",
        "pageIndex": "1",
        "pageSize": "15",
        "public": "false",
        "infoLevel": info_level,
        "sign": sign,
        "channelIdStr": "2496,2556,2632,2663,2589,2850,3300,",  # å»æ‰ç©ºæ ¼
        "nodeLevel": "1",
        "op": "top",  # ç¬¬ä¸€é¡µç”¨topï¼Œåç»­é¡µç”¨next
        "searchAfter": "",  # æ¸¸æ ‡åˆ†é¡µå‚æ•°
    }
    
    if USE_DATE_RANGE:
        print(f"ä½¿ç”¨æ—¥æœŸèŒƒå›´æŸ¥è¯¢: {START_DATE} åˆ° {END_DATE}")

    # Define headers for the request
    # headers = {
    #     "Content-Type": "application/x-www-form-urlencoded",
    #     "Accept": "application/json, text/javascript, */*; q=0.01",
    #     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36",
    # }
    # Define headers for the request
    # def get_headers(url):
    #     headers = {
    #         "Content-Type": "application/x-www-form-urlencoded",
    #         "Accept": "application/json, text/javascript, */*; q=0.01",
    #         "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36",
    #         "Referer": url,
    #     }
    #     return headers

    # headers = get_headers('https://www.chinamoney.com.cn/chinese/qwjsn/?')
    # Define headers for the request (ç§»é™¤å›ºå®šCookieï¼Œè®©Sessionè‡ªåŠ¨ç®¡ç†)
    headers = {
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Accept-Language": "en-GB,en-US;q=0.9,en;q=0.8",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "Content-Length": "0",
        "Host": "www.chinamoney.com.cn",
        "Origin": "https://www.chinamoney.com.cn",
        "Pragma": "no-cache",
        "Referer": "https://www.chinamoney.com.cn/chinese/qwjsn/?searchValue=%25E4%25B8%258A%25E5%25B8%2582%25E6%25B5%2581%25E9%2580%259A%2520ABN",
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-origin",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        "X-Requested-With": "XMLHttpRequest",
        "sec-ch-ua": '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
    }
    requests.packages.urllib3.disable_warnings()
    shortnameList = []
    urlList = []
    
    # ä¸å†ä» abn_urls.txt åŠ è½½æ—§URLï¼Œæ¯æ¬¡éƒ½æ˜¯å…¨æ–°è·å–
    # abn_urls.txt ä»…ç”¨äºä¿å­˜æœ¬æ¬¡è·å–çš„URLï¼Œæ–¹ä¾¿è°ƒè¯•
    print("å¼€å§‹è·å–æ–°çš„URLåˆ—è¡¨...")

    # åˆå§‹åŒ–ä¸ºå½“å‰timestampï¼Œåç»­ä¼šæ›´æ–°ä¸ºå®é™…æœ€æ–°æ—¶é—´
    latestUpdateTime = parse(timestamp)
    has_new_data = False  # æ ‡è®°æ˜¯å¦æœ‰æ–°æ•°æ®

    # å¢åŠ ç¿»é¡µæ•°é‡ä»¥æ”¯æŒé•¿æ—¶é—´æœªè¿è¡Œçš„æƒ…å†µ
    # æµ‹è¯•é˜¶æ®µï¼š10é¡µ Ã— 15æ¡ = 150æ¡æ•°æ®
    # æ­£å¼è¿è¡Œï¼š100é¡µ Ã— 15æ¡ = 1500æ¡æ•°æ®
    MAX_PAGES = 100  # æŠ“å–å‰100é¡µ
    search_after = ""  # æ¸¸æ ‡åˆ†é¡µå‚æ•°
    
    for pageIndex in range(1, MAX_PAGES + 1):
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨è·å–ç¬¬ {pageIndex} é¡µæ•°æ®...")
        print(f"{'='*60}")
        
        # æ›´æ–°åˆ†é¡µå‚æ•°
        params["pageIndex"] = str(pageIndex)
        params["op"] = "top" if pageIndex == 1 else "next"
        params["searchAfter"] = search_after
        
        if USE_DATE_RANGE:
            print(f"[åˆ†é¡µå‚æ•°] pageIndex={pageIndex}, op={params['op']}, date={START_DATE}~{END_DATE}, searchAfter='{search_after[:50] if search_after else ''}'")
        else:
            print(f"[åˆ†é¡µå‚æ•°] pageIndex={pageIndex}, op={params['op']}, searchAfter='{search_after[:50] if search_after else ''}'")

        # First try without using proxy to save costs
        # ç‰¹æ®Šçš„POSTè¯·æ±‚ï¼šå‚æ•°åœ¨URLä¸­ï¼ˆparamsï¼‰ï¼Œä½†ä½¿ç”¨POSTæ–¹æ³•ï¼Œè¯·æ±‚ä½“ä¸ºç©ºï¼ˆContent-Length: 0ï¼‰
        print(f"Attempting to post to {baseurl} without proxy...")
        r = session.post(baseurl, params=params, headers=headers, verify=False)

        status_code = r.status_code
        print(f"HTTP Status Code: {status_code}")

        # Check if this is an IP-related failure (403, 429, 451)
        ip_related_failure = False
        if status_code in [403, 429, 451]:
            ip_related_failure = True
            print(f"Received {status_code} error - likely IP restriction")
        elif status_code != 200:
            # Check response text for IP blocking indicators
            try:
                response_text = r.text.lower()
                ip_block_indicators = [
                    "blocked",
                    "forbidden",
                    "access denied",
                    "åœ°å€è¢«ç¦æ­¢",
                    "è®¿é—®å—é™",
                    "è®¿é—®è¢«æ‹’ç»",
                ]
                for indicator in ip_block_indicators:
                    if indicator.lower() in response_text:
                        ip_related_failure = True
                        print(f"Detected IP blocking indicator: {indicator}")
                        break
            except:
                pass

        # If IP-related failure, retry with proxy (only if proxy is configured)
        if ip_related_failure:
            if USE_PROXY and proxies is not None:
                print("Retrying with SmartProxy due to IP restriction...")
                try:
                    r = session.post(
                        baseurl,
                        params=params,
                        headers=headers,
                        proxies=proxies,
                        verify=False,
                    )
                    status_code = r.status_code
                    print(f"HTTP Status Code with proxy: {status_code}")
                except Exception as e:
                    print(f"Error using proxy: {e}")
                    raise
            else:
                print("IP restriction detected but no proxy configured. Cannot retry.")
                print(f"Response text (first 500 chars): {r.text[:500]}")
                raise Exception(
                    f"IP restriction detected (HTTP {status_code}) and no proxy available"
                )

        # Check if we got a valid response
        if status_code != 200:
            print(f"Failed with status code: {status_code}")
            print(f"Response text (first 500 chars): {r.text[:500]}")
            raise Exception(f"Failed to fetch data: HTTP {status_code}")

        # Try to parse JSON response
        try:
            requestsdata = r.json()
        except json.JSONDecodeError as e:
            print(f"Failed to parse JSON response. Status: {status_code}")
            print(f"Response text (first 1000 chars): {r.text[:1000]}")
            raise Exception(f"Invalid JSON response from server: {e}")
        
        # è¯¦ç»†æ‰“å°å“åº”ç»“æ„
        print(f"Response keys: {requestsdata.keys()}")
        if "data" not in requestsdata:
            print(f"âŒ å“åº”ä¸­æ²¡æœ‰'data'å­—æ®µï¼")
            print(f"å®Œæ•´å“åº”: {requestsdata}")
            raise Exception("APIå“åº”æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘'data'å­—æ®µ")
        
        data = requestsdata["data"]
        print(f"Data keys: {data.keys()}")
        
        if "result" not in data:
            print(f"âŒ dataä¸­æ²¡æœ‰'result'å­—æ®µï¼")
            print(f"å®Œæ•´data: {data}")
            raise Exception("APIå“åº”æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘'result'å­—æ®µ")
        
        result = data["result"]
        print(f"Result keys: {result.keys()}")
        
        if "pageItems" not in result:
            print(f"âŒ resultä¸­æ²¡æœ‰'pageItems'å­—æ®µï¼")
            print(f"å®Œæ•´result: {result}")
            raise Exception("APIå“åº”æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘'pageItems'å­—æ®µ")
        
        pageItems = result["pageItems"]
        
        # æå–searchAfterç”¨äºä¸‹ä¸€é¡µï¼ˆæ¸¸æ ‡åˆ†é¡µï¼‰
        # searchAfteræ¥è‡ªå“åº”çš„sortNextå­—æ®µ
        search_after_found = False

        # ä»resultä¸­çš„sortNextå­—æ®µè·å–
        if "sortNext" in result:
            sort_next = result["sortNext"]
            # sortNextæ˜¯ä¸€ä¸ªæ•°ç»„ï¼š[timestamp, score, id]
            if isinstance(sort_next, list) and len(sort_next) == 3:
                # è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                search_after = ",".join(str(x) for x in sort_next)
                print(f"[åˆ†é¡µ] ä»sortNextè·å–åˆ°searchAfter: '{search_after}'")
                print(f"  sortNextæ•°ç»„: {sort_next}")
                search_after_found = True

        if not search_after_found:
            print(f"[åˆ†é¡µ] æœªæ‰¾åˆ°sortNextå­—æ®µï¼Œä¿æŒsearchAfterä¸ºç©º")
            print(f"[è°ƒè¯•] resultä¸­çš„å­—æ®µ: {result.keys()}")

        if not pageItems:
            print("âš ï¸ å½“å‰é¡µæ²¡æœ‰æ•°æ®ï¼ˆpageItemsä¸ºç©ºåˆ—è¡¨ï¼‰")
            print(f"å®Œæ•´result: {result}")
            break

        print(f"å½“å‰é¡µæœ‰ {len(pageItems)} æ¡æ•°æ®")
        page_new_count = 0  # å½“å‰é¡µæ–°æ•°æ®è®¡æ•°
        
        for i in pageItems:
            release_date_ts = int(i["releaseDate"]) / 1000
            release_date = datetime.fromtimestamp(release_date_ts)

            # å§‹ç»ˆæ›´æ–°latestUpdateTimeä¸ºé‡åˆ°çš„æœ€æ–°æ—¶é—´ï¼ˆæ— è®ºæ˜¯å¦å¤„ç†ï¼‰
            if release_date > latestUpdateTime:
                latestUpdateTime = release_date

            # æ—¥æœŸèŒƒå›´æ¨¡å¼ï¼šæ”¶é›†æ‰€æœ‰æ•°æ®ï¼›æ—¶é—´æˆ³æ¨¡å¼ï¼šåªå¤„ç†æ¯”ä¸Šæ¬¡æ›´æ–°æ—¶é—´æ›´æ–°çš„æ•°æ®
            should_process = USE_DATE_RANGE or (release_date > parse(timestamp))
            
            if should_process:
                has_new_data = True
                page_new_count += 1
                title = (
                    i["title"][9:-1]
                    .replace("<font color='red'>", "")
                    .replace("</font>", "")
                )
                

                
                # FILTER REMOVED: Match is unreliable on Title (Code vs Name). Filtering in newProd instead.
                # is_match, _ = is_target_match(title)
                # if not is_match:
                #    continue

                short_name_match = re.findall("\d{2}.+?\d{3}", title)
                if short_name_match:
                    short_name = short_name_match[0]
                else:
                    # Fallback: Use full title if short name pattern not found
                    # This ensures products like "ä¿åˆ©å•†ä¸šä¿ç†..." are not skipped
                    print(f"  âš ï¸ Non-standard regex match for title: {title}. Using title as identifier.")
                    short_name = title

                if short_name not in shortnameList:
                    shortnameList.append(short_name)
                    urlList.append(
                        short_name
                        + ":"
                        + i["dealPath"]
                        + ":"
                        + release_date.strftime("%Y-%m-%d")
                    )
                    if short_name_match:
                        print(f"  âœ“ ğŸ¯ ç›®æ ‡æ–°äº§å“: {short_name} (æ—¥æœŸ: {release_date.strftime('%Y-%m-%d')})")
                    else:
                        print(f"  âœ“ ğŸ¯ ç›®æ ‡æ–°äº§å“(éæ ‡å): {short_name[:20]}... (æ—¥æœŸ: {release_date.strftime('%Y-%m-%d')})")
                else:
                    print(f"  âŠ— è·³è¿‡é‡å¤: {short_name}")
            # ä¸è¦breakï¼Œç»§ç»­å¤„ç†å½“å‰é¡µçš„å…¶ä»–é¡¹ç›®
        
        print(f"ç¬¬ {pageIndex} é¡µ: å…± {len(pageItems)} æ¡æ•°æ®ï¼Œå…¶ä¸­æ–°æ•°æ® {page_new_count} æ¡")

        # å¦‚æœå½“å‰é¡µæ²¡æœ‰æ–°æ•°æ®ï¼Œä¸”å·²ç»ç¿»äº†è‡³å°‘5é¡µï¼Œå¯ä»¥åœæ­¢ç¿»é¡µ
        # è¿™æ ·å¯ä»¥é¿å…å› ä¸ºç¬¬1é¡µæ°å¥½éƒ½æ˜¯æ—§æ•°æ®è€Œè¿‡æ—©åœæ­¢
        if not has_new_data and pageIndex >= 5:
            print(f"No new data found on page {pageIndex}, stopping pagination")
            break
        
        has_new_data = False  # é‡ç½®æ ‡è®°ï¼Œæ£€æŸ¥ä¸‹ä¸€é¡µ
        
        # sleep for a while (å¢åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«)
        if pageIndex < MAX_PAGES:  # ä¸æ˜¯æœ€åä¸€é¡µæ‰å»¶è¿Ÿ
            delay = random.uniform(2, 4)
            print(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
            time.sleep(delay)

    print(f"\n{'='*80}")
    print(f"æ•°æ®è·å–å®Œæˆï¼")
    print(f"{'='*80}")
    print(f"æ‰¾åˆ°çš„URLæ€»æ•°: {len(urlList)}")
    print(f"ä¸Šæ¬¡æ›´æ–°æ—¶é—´: {timestamp}")
    print(f"æœ¬æ¬¡æœ€æ–°æ—¶é—´: {latestUpdateTime}")
    print(f"æ—¶é—´å·®: {(latestUpdateTime - parse(timestamp)).days} å¤©")
    print(f"{'='*80}\n")
    return latestUpdateTime, urlList


# post_list_data() å·²åˆ é™¤ - ä½¿ç”¨ä»£ç†æ± çš„æ—§å®ç°ï¼Œå·²è¢«æ™ºèƒ½ä»£ç†ç­–ç•¥æ›¿ä»£


# ç›®å‰åªæŠ½å–å‹Ÿé›†è¯´æ˜ä¹¦ä¸èµ„äº§è¿è¥æŠ¥å‘Š
def get_usefulPDF(pdf_list):
    q1 = []
    q2 = []
    exist = []
    p = re.compile("ç¬¬.+?æœŸ")
    done = 0
    for i in pdf_list:
        if "å‹Ÿé›†è¯´æ˜ä¹¦" in i[1] and done == 0:
            done = 1
            q1.append(i)
        if "å‘è¡Œ" in i[1]:
            q1.append(i)
        # if 'èµ„äº§è¿è¥æŠ¥å‘Š' in i[1]:
        #     q = p.findall(i[1].split('èµ„äº§è¿è¥æŠ¥å‘Š')[-1])
        #     if q:
        #         if q[-1] in exist:
        #             continue
        #         exist.append(q[-1])
        #         q2.append(i)
    return q1, q2


def get_file_list(product_name):
    print("getting file list for", product_name)

    # è·å–åŠ¨æ€tokenï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆæé«˜å®¹é”™æ€§
    token = get_token()
    if token is None:
        print("è­¦å‘Š: æ— æ³•è·å–tokenï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
        sign, info_level = get_sign_and_info_level()
        if sign is None or info_level is None:
            print("Failed to get token for file list")
            return []
    else:
        info_level = token["UT"].replace("\n", "")
        sign = token["sign"].replace("\n", "")

    url = "https://www.chinamoney.com.cn/ses/rest/cm-u-notice-ses-cn/query"
    data = {
        "sort": "date",
        "text": product_name,
        "date": "all",
        "field": "title",
        "start": "",
        "end": "",
        "pageIndex": "1",
        "pageSize": "15",
        "public": "false",
        "infoLevel": info_level,
        "sign": sign,
        "channelIdStr": "2496,2556,2632,2663,2589,2850,3300,",
        "nodeLevel": "1",
        "op": "top",
        "searchAfter": "",
    }
    # ä½¿ç”¨Sessionç®¡ç†Cookieï¼ˆä¸è¦ç¡¬ç¼–ç Cookieï¼‰
    session = requests.Session()
    
    # å…ˆè®¿é—®ä¸»é¡µè·å–æœ‰æ•ˆCookie
    try:
        home_url = "https://www.chinamoney.com.cn/chinese/qwjsn/"
        home_headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Encoding": "gzip, deflate, br, zstd",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        }
        session.get(home_url, headers=home_headers, verify=False, timeout=10)
        print(f"è·å–Cookie: {session.cookies.get_dict()}")
    except Exception as e:
        print(f"è®¿é—®ä¸»é¡µè·å–Cookieå¤±è´¥: {e}")
    
    # APIè¯·æ±‚å¤´ï¼ˆä¸åŒ…å«Cookieï¼Œè®©Sessionè‡ªåŠ¨ç®¡ç†ï¼‰
    headers = {
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Accept-Language": "en-GB,en-US;q=0.9,en;q=0.8",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "Content-Length": "0",
        "Host": "www.chinamoney.com.cn",
        "Origin": "https://www.chinamoney.com.cn",
        "Pragma": "no-cache",
        "Referer": "https://www.chinamoney.com.cn/chinese/qwjsn/?searchValue=%25E4%25B8%258A%25E5%25B8%2582%25E6%25B5%2581%25E9%2580%259A%2520ABN",
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-origin",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        "X-Requested-With": "XMLHttpRequest",
        "sec-ch-ua": '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
    }

    # First try without using proxy to save costs
    print("Attempting to fetch file list without proxy...")
    rs = session.post(url, params=data, headers=headers, verify=False)
    time.sleep(4.33)
    print("rs.status =", rs.status_code)

    # Check if this is an IP-related failure
    ip_related_failure = False
    if rs.status_code in [403, 429, 451]:
        ip_related_failure = True
        print(f"Received {rs.status_code} error - likely IP restriction")
    elif rs.status_code != 200:
        try:
            response_text = rs.text.lower()
            ip_block_indicators = [
                "blocked",
                "forbidden",
                "access denied",
                "åœ°å€è¢«ç¦æ­¢",
                "è®¿é—®å—é™",
                "è®¿é—®è¢«æ‹’ç»",
            ]
            for indicator in ip_block_indicators:
                if indicator.lower() in response_text:
                    ip_related_failure = True
                    print(f"Detected IP blocking indicator: {indicator}")
                    break
        except:
            pass

    # If IP-related failure, retry with proxy (only if proxy is configured)
    if ip_related_failure:
        if USE_PROXY and proxies is not None:
            print("Retrying with SmartProxy due to IP restriction...")
            rs = session.post(url, params=data, headers=headers, proxies=proxies, verify=False)
            time.sleep(4.33)
            print("rs.status with proxy =", rs.status_code)
        else:
            print("IP restriction detected but no proxy configured. Cannot retry.")
            print(f"Response text (first 500 chars): {rs.text[:500]}")
            raise Exception(
                f"IP restriction detected (HTTP {rs.status_code}) and no proxy available"
            )

    # Check for errors
    if rs.status_code != 200:
        print(f"Error: HTTP {rs.status_code}")
        print(f"Response text (first 500 chars): {rs.text[:500]}")
        raise Exception(f"Failed to fetch file list: HTTP {rs.status_code}")

    try:
        rs = json.loads(rs.text)
    except json.JSONDecodeError as e:
        print(f"Failed to parse JSON response")
        print(f"Response text (first 1000 chars): {rs.text[:1000]}")
        raise Exception(f"Invalid JSON response: {e}")

    total = rs["data"]["result"]["total"]
    print(f"æœç´¢ç»“æœæ€»æ•°: {total}")

    if total > 50:
        time.sleep(random.uniform(1.5, 2.9))
        data["pageSize"] = total

        # First try without proxy
        print(f"Fetching full list ({total} items) without proxy...")
        rs = session.post(url, params=data, headers=headers, verify=False)
        time.sleep(3.156)

        # Check if this is an IP-related failure
        ip_related_failure = False
        if rs.status_code in [403, 429, 451]:
            ip_related_failure = True
            print(
                f"Received {rs.status_code} error on second request - likely IP restriction"
            )
        elif rs.status_code != 200:
            try:
                response_text = rs.text.lower()
                ip_block_indicators = [
                    "blocked",
                    "forbidden",
                    "access denied",
                    "åœ°å€è¢«ç¦æ­¢",
                    "è®¿é—®å—é™",
                    "è®¿é—®è¢«æ‹’ç»",
                ]
                for indicator in ip_block_indicators:
                    if indicator.lower() in response_text:
                        ip_related_failure = True
                        print(
                            f"Detected IP blocking indicator on second request: {indicator}"
                        )
                        break
            except:
                pass

        # If IP-related failure, retry with proxy (only if proxy is configured)
        if ip_related_failure:
            if USE_PROXY and proxies is not None:
                print(
                    "Retrying second request with SmartProxy due to IP restriction..."
                )
                rs = session.post(url, params=data, headers=headers, proxies=proxies, verify=False)
                time.sleep(3.156)
                print("rs.status with proxy (second request) =", rs.status_code)
            else:
                print(
                    "IP restriction detected on second request but no proxy configured. Cannot retry."
                )
                print(f"Response text (first 500 chars): {rs.text[:500]}")
                raise Exception(
                    f"IP restriction detected (HTTP {rs.status_code}) and no proxy available"
                )

        # Check for errors
        if rs.status_code != 200:
            print(f"Error on second request: HTTP {rs.status_code}")
            print(f"Response text (first 500 chars): {rs.text[:500]}")
            raise Exception(f"Failed to fetch full file list: HTTP {rs.status_code}")

        try:
            rs = json.loads(rs.text)
        except json.JSONDecodeError as e:
            print(f"Failed to parse JSON response on second request")
            print(f"Response text (first 1000 chars): {rs.text[:1000]}")
            raise Exception(f"Invalid JSON response: {e}")

    pdf_list = []
    pageItems = rs["data"]["result"]["pageItems"]
    print(f"pageItemsæ•°é‡: {len(pageItems)}")
    for i in pageItems:
        title = i["title"].replace("<font color='red'>", "").replace("</font>", "")
        
        if len(i["paths"]) == 1:
            # æœ‰PDFé™„ä»¶ï¼Œä»pathsä¸­æå–æ—¥æœŸ
            z = re.search("\d{4}/?\d{2}/?\d{2}", i["paths"][0]).group().replace("/", "")
            date = z[:4] + "-" + z[4:6] + "-" + z[6:8] + " 00:00:00.000"
            pdf_list.append([i["id"], title, date])
        elif len(i["paths"]) == 0:
            # pathsä¸ºç©º = çº¯æ–‡æœ¬å…¬å‘Šï¼Œæ²¡æœ‰PDFé™„ä»¶ï¼Œè·³è¿‡
            print(f"  è·³è¿‡(æ— PDF): {title}")
        else:
            print(f"  è·³è¿‡(paths={len(i['paths'])}): {title}")
    
    print(f"æœ‰æ•ˆPDFæ–‡æ¡£æ•°: {len(pdf_list)}")
    return pdf_list


# get_url_list() å·²åˆ é™¤ - æ—§ç‰ˆå®ç°ï¼Œå·²è¢« get_html() æ›¿ä»£


# upload_dir() å·²åˆ é™¤ - æ—§ç‰ˆFTPä¸Šä¼ ï¼Œåªè¢«å·²åˆ é™¤çš„ upload_211_bak() ä½¿ç”¨


# upload_folder_to_ftp() å·²åˆ é™¤ - æœ‰bugä¸”ä»æœªè¢«è°ƒç”¨



def get_file_data_from_ftp_with_retry(ftp, ftp_file_path, retries=5):
    for attempt in range(retries):
        try:
            bio = io.BytesIO()
            ftp.retrbinary(f"RETR {ftp_file_path}", bio.write)
            bio.seek(0)
            return bio
        except (ftplib.all_errors, EOFError, socket.timeout, ConnectionError, BrokenPipeError, OSError) as e:
            print(f"Error getting file data from FTP (Attempt {attempt+1}/{retries}): {e}")
            if attempt < retries - 1:
                 print("Reconnecting and retrying download...")
                 try:
                     ftp.close()
                 except: pass
                 try:
                    ftp.connect(FTP_HOST, FTP_PORT, timeout=120)
                    ftp.login(FTP_USER, FTP_PASS)
                 except Exception as re:
                     print(f"Reconnection failed: {re}")
                 time.sleep(5)
    raise Exception(f"Failed to get file data for {ftp_file_path} after retries")


def write_file_data_to_ftp_with_retry(ftp, file_data, ftp_file_path, retries=5):
    for attempt in range(retries):
        try:
            file_data.seek(0) # IMPORTANT: Reset pointer before upload
            ftp.storbinary(f"STOR {ftp_file_path}", file_data)
            return
        except (ftplib.all_errors, EOFError, socket.timeout, ConnectionError, BrokenPipeError, OSError) as e:
            print(f"Error writing file data to FTP (Attempt {attempt+1}/{retries}): {e}")
            if attempt < retries - 1:
                 print("Reconnecting and retrying upload...")
                 try:
                     ftp.close()
                 except: pass
                 try:
                    ftp.connect(FTP2_HOST, FTP2_PORT, timeout=120)
                    ftp.login(FTP2_USER, FTP2_PASS)
                    enable_utf8(ftp) # Ensure UTF8 is enabled for FTP2
                 except Exception as re:
                     print(f"Reconnection failed: {re}")
                 time.sleep(5)
    raise Exception(f"Failed to write file data for {ftp_file_path} after retries")


def upload_ftp_folder_to_ftp(ftp1, ftp2, ftp1_folder, ftp2_folder):
    for item in list_ftp_directory_with_retry(ftp1, ftp1_folder):
        ftp1_item_path = os.path.join(ftp1_folder, item).replace("\\", "/")
        ftp2_item_path = os.path.join(ftp2_folder, item).replace("\\", "/")

        # check if ftp1_item_path is a folder
        if "." in ftp1_item_path:
            print(f"Transferring file: {ftp1_item_path} to {ftp2_item_path}")
            try:
                # Use retry functions
                file_data = get_file_data_from_ftp_with_retry(ftp1, ftp1_item_path)
                write_file_data_to_ftp_with_retry(ftp2, file_data, ftp2_item_path)
            except Exception as e:
                 print(f"Failed to transfer file {ftp1_item_path}: {e}")
                 # Continue to next item? Or raise?
                 # Probably safer to continue but log error
                 pass
        else:
            # Check if item is a directory on ftp1
            ftp1.cwd(ftp1_item_path)

            try:
                print(f"Creating directory: {ftp2_item_path}")
                ftp2.mkd(ftp2_item_path)
                ftp2.cwd(ftp2_item_path)
            except ftplib.error_perm:
                # Directory might already exist, ignore error
                pass
            # Recursively upload folder
            upload_ftp_folder_to_ftp(ftp1, ftp2, ftp1_item_path, ftp2_item_path)

        # try:
        #     # Check if item is a directory on ftp1
        #     ftp1.cwd(ftp1_item_path)
        #     ftp2.cwd(ftp2_item_path)
        # except ftplib.error_perm:
        #     # Item is a file
        #     print(f"Transferring file: {ftp1_item_path} to {ftp2_item_path}")
        #     file_data = get_file_data_from_ftp(ftp1, ftp1_item_path)
        #     write_file_data_to_ftp(ftp2, file_data, ftp2_item_path)
        # else:
        #     # Item is a directory
        #     print(f"Creating directory: {ftp2_item_path}")
        #     try:
        #         ftp2.mkd(ftp2_item_path)
        #     except ftplib.error_perm:
        #         # Directory might already exist, ignore error

def upload_211(trustcode):
    month = str(datetime.now().month)
    folder = create_dir_on_ftp(ftp2, "/å¢é‡æ–‡æ¡£", month)
    source_folder = create_dir_on_ftp(ftp2, folder, trustcode)

    target_folder = create_dir_on_ftp(ftp2, "/DealViewer/TrustAssociatedDoc", trustcode)

    upload_ftp_folder_to_ftp(ftp, ftp2, source_folder, target_folder)


# upload_211_bak() å·²åˆ é™¤ - å¤‡ä»½å‡½æ•°ï¼Œä½¿ç”¨æ—§çš„ç½‘ç»œå…±äº«è·¯å¾„


def crawl_pdf(product_name, i, type_f):
    pdf_file_name = i[1] + ".pdf"

    cache_pdf_path = os.path.join(cache_folder, pdf_file_name)
    file_exists = os.path.exists(cache_pdf_path)

    month = str(datetime.now().month)

    if not file_exists:
        url = static_url.format(int(i[0]))

        # ä½¿ç”¨æ™ºèƒ½ä»£ç†ç­–ç•¥ä¸‹è½½PDF
        try:
            print("downloading file from url:", url)
            success, content = get_web_pdf_content_with_retry(url)
            if success:
                with open(cache_pdf_path, "wb") as f:
                    f.write(content)
                file_exists = True
                print(f"âœ“ PDFä¸‹è½½æˆåŠŸ: {pdf_file_name}")
            else:
                print(f"âœ— PDFä¸‹è½½å¤±è´¥: {pdf_file_name}, åŸå› : {content}")
        except Exception as e:
            print(f"âœ— PDFä¸‹è½½å¼‚å¸¸: {pdf_file_name}, é”™è¯¯: {e}")
            time.sleep(random.uniform(1, 5))


def upload_file_to_ftp_with_retry(ftp, local_file_path, ftp_folder, ftp_file_path, file_name, retries=5):
    """Upload file with retry logic for EOFError and connection issues"""
    # Check if exists
    try:
        # Avoid checking if we are sure? No, logic requires check.
        if file_name in list_ftp_directory_with_retry(ftp, ftp_folder):
            print(f"File already exists on FTP: {ftp_file_path}")
            return
    except Exception as e:
        print(f"Error checking file existence during upload: {e}")
        # Try to proceed anyway?

    for attempt in range(retries):
        try:
            print(f"Writing PDF to {ftp_file_path} (Attempt {attempt+1}/{retries})")
            
            # Ensure connection is alive?
            try:
                ftp.voidcmd("NOOP")
            except:
                print("Connection lost, reconnecting before upload...")
                try:
                    ftp.close()
                except:
                    pass
                ftp.connect(FTP_HOST, FTP_PORT, timeout=120)
                ftp.login(FTP_USER, FTP_PASS)

            with open(local_file_path, "rb") as f:
                ftp.storbinary(f"STOR {ftp_file_path}", f)
            print(f"âœ“ Upload successful: {file_name}")
            return
        except (ftplib.all_errors, EOFError, socket.timeout, ConnectionError, BrokenPipeError, OSError) as e:
            print(f"Upload failed: {e}")
            if attempt < retries - 1:
                print("Reconnecting and retrying...")
                try:
                    ftp.close()
                except: pass
                try:
                    ftp.connect(FTP_HOST, FTP_PORT, timeout=120)
                    ftp.login(FTP_USER, FTP_PASS)
                except Exception as re:
                     print(f"Reconnection failed: {re}")
                time.sleep(5)
    
    print(f"âŒ Failed to upload {file_name} after retries")


    if file_exists:
        ftp_folder = create_dir_on_ftp(ftp, "/Products/èµ„äº§æ”¯æŒç¥¨æ®", product_name[2])
        ftp_file_path = os.path.join(ftp_folder, pdf_file_name).replace("\\", "/")

        upload_file_to_ftp_with_retry(
            ftp, cache_pdf_path, ftp_folder, ftp_file_path, pdf_file_name
        )

        if type_f == 1:
            create_dir_on_ftp(ftp, "/å¢é‡æ–‡æ¡£", month)
            folder = create_dir_on_ftp(ftp, f"/å¢é‡æ–‡æ¡£/{month}", product_name[1])
            ftp_folder = create_dir_on_ftp(ftp, folder, "ProductReleaseInstructions")
            ftp_file_path = os.path.join(ftp_folder, pdf_file_name).replace("\\", "/")
            upload_file_to_ftp_with_retry(
                ftp, cache_pdf_path, ftp_folder, ftp_file_path, pdf_file_name
            )

        if type_f == 2:
            create_dir_on_ftp(ftp, "/å¢é‡æ–‡æ¡£", month)
            folder = create_dir_on_ftp(ftp, f"/å¢é‡æ–‡æ¡£/{month}", product_name[1])
            ftp_folder = create_dir_on_ftp(ftp, folder, "TrusteeReport")
            ftp_file_path = os.path.join(ftp_folder, pdf_file_name).replace("\\", "/")
            upload_file_to_ftp_with_retry(
                ftp, cache_pdf_path, ftp_folder, ftp_file_path, pdf_file_name
            )

    # with open(r'\\172.16.7.114\/Products/èµ„äº§æ”¯æŒç¥¨æ®\{}\{}.pdf'.format(product_name[2], i[1]), 'wb') as f:
    #     f.write(rs.content)
    #     print(i[1] + '.PDF done')

    # if type_f == 1:
    #     with open(
    #             r'\\172.16.7.114\/å¢é‡æ–‡æ¡£\{}\{}\ProductReleaseInstructions\{}.pdf'.format(
    #                 str(datetime.now().month), product_name[1],
    #                 i[1]),
    #             'wb') as f:
    #         f.write(rs.content)
    # if type_f == 2:
    #     with open(r'\\172.16.7.114\/å¢é‡æ–‡æ¡£\{}\{}\TrusteeReport\{}.pdf'.format(str(datetime.now().month),
    #                                                                                 product_name[1], i[1]),
    #               'wb') as f:
    #         f.write(rs.content)

    time.sleep(random.uniform(1.5, 2.9))


# crawl_pdf_bak() å·²åˆ é™¤ - å¤‡ä»½å‡½æ•°ï¼Œä½¿ç”¨æ—§çš„ç½‘ç»œå…±äº«è·¯å¾„


def upload_114(product_name, q1, q2):
    print(product_name)
    month = str(datetime.now().month)
    if any([q1, q2]):
        print("creating folder for", product_name[2])
        create_dir_on_ftp(ftp, "/Products/èµ„äº§æ”¯æŒç¥¨æ®", product_name[2])
        folder = create_dir_on_ftp(ftp, "/å¢é‡æ–‡æ¡£", month)
        create_dir_on_ftp(ftp, folder, product_name[1])

        # if not os.path.exists(r'\\172.16.7.114\/Products/èµ„äº§æ”¯æŒç¥¨æ®\{}'.format(product_name[2])):
        #     os.mkdir(r'\\172.16.7.114\/Products/èµ„äº§æ”¯æŒç¥¨æ®\{}'.format(product_name[2]))
        # if not os.path.exists(r'\\172.16.7.114\/å¢é‡æ–‡æ¡£\{}\{}'.format(str(datetime.now().month), product_name[1])):
        #     os.mkdir(r'\\172.16.7.114\/å¢é‡æ–‡æ¡£\{}\{}'.format(str(datetime.now().month), product_name[1]))

    if q1:
        create_dir_on_ftp(
            ftp, f"/å¢é‡æ–‡æ¡£/{month}/{product_name[1]}", "ProductReleaseInstructions"
        )
        # if not os.path.exists((r'\\172.16.7.114\/å¢é‡æ–‡æ¡£\{}\{}\ProductReleaseInstructions'.format(str(datetime.now().month),
        #                                                                                  product_name[1]))):
        #     os.mkdir(r'\\172.16.7.114\/å¢é‡æ–‡æ¡£\{}\{}\ProductReleaseInstructions'.format(str(datetime.now().month),
        #                                                                                  product_name[1]))

        # if any([q1, q2]):
        #     os.mkdir(r'\\172.16.7.114\/Products/èµ„äº§æ”¯æŒç¥¨æ®\{}'.format(product_name[2]))
        #     os.mkdir(r'\\172.16.7.114\/å¢é‡æ–‡æ¡£\{}\{}'.format(str(datetime.now().month), product_name[1]))
        #
        # if q1:
        #     os.mkdir(r'\\172.16.7.114\/å¢é‡æ–‡æ¡£\{}\{}\ProductReleaseInstructions'.format(str(datetime.now().month),
        #                                                                                      product_name[1]))
        for i in q1:
            print(i)
            crawl_pdf(product_name, i, 1)
    # if q2:
    #     os.mkdir(r'\\172.16.7.114\/å¢é‡æ–‡æ¡£\{}\{}\TrusteeReport'.format(str(datetime.now().month), product_name[1]))
    #     for i in q2:
    #         print(i)
    #         crawl_pdf(product_name, i, 2)

    # for i in q1:
    #     print(i)
    #     crawl_pdf(product_name, i, 1)


def newProd(url):
    print("newProd process", url)
    
    # Pre-fetch check (Url is partial, but maybe useful to log)
    # log_project_event(url, "Starting newProd process")

    df = pd.DataFrame()
    url = "https://www.chinamoney.com.cn" + url

    # æ£€æŸ¥url_done.txtæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    url_done_file = "url_done.txt"
    if os.path.exists(url_done_file):
        with open(url_done_file, "r", encoding="utf8") as f:
            done_urls = f.read().splitlines()
            if url in done_urls:
                print("æ–‡ä»¶åˆ¤æ–­æ­¤urlå·²å¤„ç†å®Œæˆ")
                return
    else:
        print("url_done.txt not found, creating new file")
        done_urls = []

    flag = 0

    retries = 5

    while flag == 0 and retries > 0:
        try:
            retries -= 1
            wd = pd.read_html(url)
            flag = 1
        except:
            print("pd.read_html failed")
            time.sleep(random.uniform(1, 5))

    df = pd.concat(wd, ignore_index=True)
    fullName = df.iloc[1][1]
    fullName = fullName.split("ç¥¨æ®")[0] + "ç¥¨æ®"
    
    # TRACKING FULLNAME REMOVED

    rawShortName = df.iloc[1][3]
    try:
        # å°è¯•è§£ææ ‡å‡†ABNç®€ç§°æ ¼å¼ (ä¾‹å¦‚: 24ä¿åˆ©ABN001)
        if "ABN" in rawShortName and len(rawShortName) > 5:
            shortName = (
                rawShortName[2 : rawShortName.find("ABN") + 3]
                + "20"
                + rawShortName[:2]
                + "-"
                + str(int(rawShortName[rawShortName.find("ABN") + 3 : rawShortName.find("ABN") + 6]))
            )
        else:
            # æ ¼å¼ä¸åŒ¹é…ï¼Œä½¿ç”¨åŸå§‹å€¼
            print(f"  âš ï¸ éæ ‡å‡†ç®€ç§°æ ¼å¼: {rawShortName}ï¼Œä½¿ç”¨åŸå§‹å€¼")
            shortName = rawShortName
    except Exception as e:
        print(f"  âš ï¸ è§£æç®€ç§°å‡ºé”™ '{rawShortName}': {e}ï¼Œä½¿ç”¨åŸå§‹å€¼")
        shortName = rawShortName
    offer_type = df.iloc[-1][3]

    # âŒ ç§»é™¤æµ‹è¯•ç”¨çš„äº§å“è¿‡æ»¤ï¼Œå¤„ç†æ‰€æœ‰äº§å“
    # if "ä¸€æ±½" not in fullName:
    #     return

    # å…¬å‹Ÿï¼šPublicOfferingï¼Œç§å‹Ÿï¼šPrivateEquity
    # åªè¦offer_typeä¸åŒ…å«"å®šå‘"æˆ–"ç§å‹Ÿ"ï¼Œå°±å±äºPublicOffering
    if "å®šå‘" in offer_type or "ç§å‹Ÿ" in offer_type:
        CollectionMethod = "PrivateEquity"
    else:
        CollectionMethod = "PublicOffering"
    
    print(f"å‘è¡Œç±»å‹: {offer_type} -> {CollectionMethod}")

    # # conn = pymssql.connect(host='172.16.6.143\mssql', user='sa', password='PasswordGS2017',
    # #                        database='PortfolioManagement', charset='utf8')
    cursor = conn.cursor()

    TrustStatus = "Duration"
    TrustName = fullName
    TrustNameShort = shortName
    TrustCode = "".join([i[0].upper() + i[1:] for i in lazy_pinyin(TrustNameShort)])

    # check if TrustName exists in database_existing_products.txt
    # trust_exists = False
    # existing_file_path = os.path.join(current_dir, 'database_existing_products.txt')
    # if os.path.exists(existing_file_path):
    #     with open('database_existing_products.txt', 'r', encoding='utf8') as f:
    #         existing_products = f.read().splitlines()
    #         if TrustName in existing_products:
    #             trust_exists = True
    #             print(TrustName, 'æ–‡ä»¶åˆ¤æ–­æ•°æ®åº“å·²å­˜åœ¨')

    #             with open('existing_urls.txt', 'a', encoding='utf8') as f:
    #                 f.write(url + '\n')

    # return

    isExist = "select 1 from TrustManagement.Trust where TrustName=N'{}' or TrustCode='{}' or TrustNameShort=N'{}'".format(
        TrustName, TrustCode, TrustNameShort
    )

    cursor.execute(isExist)
    res = cursor.fetchone()
    if res:
        print(TrustName, "æ•°æ®åº“å·²å­˜åœ¨")


        with open("æ ¸æŸ¥å†²çªäº§å“.txt", "a", encoding="utf8") as f:
            f.write(df.iloc[1][3] + " ")
            f.write(TrustName + " ")
            f.write(TrustCode + " ")
            f.write(TrustNameShort + "\n")
        # return
    else:
        selectTrustId = (
            "select max(TrustId)+1 from TrustManagement.Trust where TrustId<50000"
        )
        cursor.execute(selectTrustId)
        TrustId = cursor.fetchone()[0]

        # æ’å…¥Trustæ•°æ®
        insertTrust = """
            SET IDENTITY_INSERT TrustManagement.Trust ON ;
            insert into TrustManagement.Trust(TrustId,TrustCode,TrustName,TrustNameShort,IsMarketProduct,TrustStatus) values({},'{}',N'{}',N'{}',1,'Duration');
            SET IDENTITY_INSERT TrustManagement.Trust OFF ;
        """.format(
            TrustId, TrustCode, TrustName, TrustNameShort
        )
        print(insertTrust)

        try:
            cursor.execute(insertTrust)
            conn.commit()
            print(TrustName, "åŸºç¡€è¡¨ä¿¡æ¯æ’å…¥å®Œæˆ!")

        except Exception as e:
            print("err1", e)

            # print(TrustName)
            # print(insertTrust)
            return

        # æ’å…¥åŒæ­¥è¡¨æ•°æ®
        insertFATrust = """
            SET IDENTITY_INSERT FixedIncomeSuite.Analysis.Trust ON ;
            insert into FixedIncomeSuite.Analysis.Trust(TrustId,TrustCode,TrustName) values({},'{}',N'{}');
            SET IDENTITY_INSERT FixedIncomeSuite.Analysis.Trust OFF ;
        """.format(
            TrustId, TrustCode, TrustName
        )
        print(insertFATrust)

        try:
            cursor.execute(insertFATrust)
            conn.commit()
            print(TrustName, "åŒæ­¥è¡¨ä¿¡æ¯æ’å…¥å®Œæˆ!")
        except Exception as e:
            print("errFA", e)
            # print(TrustName)
            # print(insertFATrust)

        # æ’å…¥TrustInfoExtensionæ•°æ®
        RegulatoryOrg = "NAFMII"
        MarketPlace = "InterBank"

        TrustInfoExtension = (
            "insert into TrustManagement.TrustInfoExtension(TrustId, StartDate, EndDate, ItemId, ItemCode, ItemValue) values "
            "({}, GETDATE(), null, null, 'MarketCategory','ABN'),"
            "({}, GETDATE(), null, null, 'RegulatoryOrg','{}'),"
            "({}, GETDATE(), null, null, 'MarketPlace', '{}'),"
            "({}, GETDATE(), null, null, 'AssetType',null),"
            "({}, GETDATE(), null, null, 'BasicAssetType',null),"
            "({}, GETDATE(), null, null, 'CollectionMethod', '{}');".format(
                TrustId,
                TrustId,
                RegulatoryOrg,
                TrustId,
                MarketPlace,
                TrustId,
                TrustId,
                TrustId,
                CollectionMethod,
            )
        )

        try:
            cursor.execute(TrustInfoExtension)
            cursor.execute(
                "update TrustManagement.TrustInfoExtension set ItemValue=null where ItemValue='nan'"
            )
            conn.commit()
            print(TrustId, TrustName, "TrustInfoExtensionè¡¨æ•°æ®æ’å…¥å®Œæˆ!")
        except Exception as e:
            print("err2", e)
            print(TrustName)
            print(TrustInfoExtension)
            return

    sql = f"select TrustId from TrustManagement.Trust where TrustName = N'{TrustName}'"
    print(f"æŸ¥è¯¢TrustId SQL: {sql}")
    cursor.execute(sql)
    result = cursor.fetchone()
    
    if result is None:
        print(f"âŒ é”™è¯¯: æ— æ³•åœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°äº§å“ '{TrustName}'")

        print(f"å¯èƒ½åŸå› : 1) äº§å“åç§°åŒ…å«ç‰¹æ®Šå­—ç¬¦ 2) æ’å…¥å¤±è´¥ä½†æœªæŠ¥é”™ 3) äº‹åŠ¡æœªæäº¤")
        # å°è¯•å†æ¬¡æŸ¥è¯¢ç¡®è®¤
        cursor.execute("select top 5 TrustId, TrustName from TrustManagement.Trust order by TrustId desc")
        recent = cursor.fetchall()
        print(f"æœ€è¿‘æ’å…¥çš„5ä¸ªäº§å“:")
        for r in recent:
            print(f"  TrustId={r[0]}, TrustName={r[1]}")
        return None
    
    TrustId = result[0]
    print(f"âœ“ æ‰¾åˆ°äº§å“ TrustId={TrustId}, TrustName={TrustName}")

    # Unconditionally process documents for all products (Public or Private)
    print(f"âœ“ å¼€å§‹å°è¯•å¤„ç†æ–‡æ¡£ (CollectionMethod={CollectionMethod})...")
    product_name = [TrustId, TrustCode, TrustName]
    file_list = get_file_list(TrustName)
    q1, q2 = get_usefulPDF(file_list)
    
    # log_project_event(TrustName, f"Found {len(q1)} release docs, {len(q2)} reports")
    
    print(f"äº§å“å‘è¡Œæ–‡æ¡£(q1): {q1}")
    print(f"è¿è¥æŠ¥å‘Šæ–‡æ¡£(q2): {q2}")
    upload_114(product_name, q1, q2)
    if q1:
        smsExist = 0
        upload_211(product_name[1])
        print(f"å¼€å§‹å¤„ç† {len(q1)} ä¸ªäº§å“å‘è¡Œæ–‡æ¡£...")
        for idx, q in enumerate(q1, 1):
            print(f"  å¤„ç†æ–‡æ¡£ {idx}/{len(q1)}: {q[1]}")
            insert_into_db(product_name, q, 1)
            if "è¯´æ˜ä¹¦" in q[1]:
                smsExist = 1
                print(f"    âœ“ åŒ…å«'è¯´æ˜ä¹¦'å…³é”®è¯")
        if not smsExist:
            print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°è¯´æ˜ä¹¦æ–‡æ¡£")
            # log_project_event(TrustName, "Warning: No 'è¯´æ˜ä¹¦' found in docs")
            gmEx.add(product_name[2])
    else:
        # Graceful handling for missing docs (common for Private products)
        print(f"âš ï¸ æç¤º: æœªæ‰¾åˆ°äº§å“å‘è¡Œæ–‡æ¡£(q1ä¸ºç©º)ï¼Œå¯èƒ½æ˜¯ç§å‹Ÿäº§å“ä¸”æ— å…¬å¼€æ–‡æ¡£")
        # log_project_event(TrustName, "No release documents found (skipped gracefully)")


    # è®°å½•å·²å¤„ç†çš„URL
    with open("url_done.txt", "a", encoding="utf8") as f:
        f.write(url + "\n")

    return offer_type, TrustName


def insert_into_db(product_name, q, typy_q):
    cursor = conn.cursor()
    # sql_i = "insert into DV.TrustAssociatedDocument(Trustid,FileCategory,SubCategory,FilePath,FileName,FileType,Created,Creator) values(%s,%s,'NULL',%s,%s,'pdf',%s,'yixianfeng')".format()
    Trustid = product_name[0]
    TrustCode = product_name[1]
    if typy_q == 1:
        FileCategory = "ProductReleaseInstructions"
        FilePath = "DealViewer/TrustAssociatedDoc/{}/{}/".format(
            TrustCode, FileCategory
        )
    if typy_q == 2:
        FileCategory = "TrusteeReport"
        FilePath = "DealViewer/TrustAssociatedDoc/{}/{}/".format(
            TrustCode, FileCategory
        )

    file_name = q[1] + ".pdf"
    created = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

    # Check if document already exists
    check_sql = f"select TrustDocumentID from DV.TrustAssociatedDocument where Trustid={Trustid} and FileName=N'{file_name}'"
    cursor.execute(check_sql)
    existing_doc = cursor.fetchone()

    if existing_doc:
        print(f"Document already exists in DV.TrustAssociatedDocument: {file_name}")
    else:
        sql_i = f"insert into DV.TrustAssociatedDocument(Trustid,FileCategory,SubCategory,FilePath,FileName,FileType,Created,Creator) values({Trustid}, '{FileCategory}','NULL','{FilePath}',N'{file_name}','pdf','{created}','goldenstand')"
        print(sql_i)
        cursor.execute(sql_i)

    print("asd")
    print(q)
    print(q[1])
    print("asd")
    
    # TRACKING: Check for Offering Circular logic
    if "å‹Ÿé›†è¯´æ˜ä¹¦" in q[1]:

        
        # sql_i1 = "insert into PortfolioManagement.dbo.DisclosureOfInformation values(%s,%s,%s,%s,%s)".format(
        # )
        # # ä¹‹å‰è®°å½•æŠ«éœ²æ—¶é—´ï¼Œç°åœ¨æ”¹æˆè·Ÿæ–‡æ¡£è¡¨ä¸€æ ·çš„å…¥åº“æ—¶é—´
        # cursor.execute(sql_i1,
        #                (Trustid, TrustCode, q[1], time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()), typy_q))

        sql_i1 = f"insert into PortfolioManagement.dbo.DisclosureOfInformation values({Trustid}, '{TrustCode}', N'{q[1]}', '{created}', {typy_q})"
        print(sql_i1)
        # ç®€å•é¿å…æ’å…¥DisclosureOfInformationæŠ¥é”™ï¼Œå¯ä»¥ç”¨try-exceptæˆ–è€…å…ˆæŸ¥ä¸€ä¸‹ï¼Œè¿™é‡Œæš‚æ—¶ä¿æŒåŸæ ·ï¼Œæˆ–è€…ä¹ŸåŠ ä¸ªcheckï¼Ÿ
        # ç”¨æˆ·ä¸»è¦å…³å¿ƒ"ä»»åŠ¡ç³»ç»Ÿé‡å¤"ï¼Œå³ProductsStateInformation
        try:
             cursor.execute(sql_i1)

        except Exception as e:
             if "2627" in str(e): # Primary key violation usually
                 pass
             else:
                 pass

        # æŸ¥è¯¢TrustDocumentID
        sql = f"select TrustDocumentID from DV.TrustAssociatedDocument where filename=N'{q[1]}.pdf'"
        cursor.execute(sql)
        result = cursor.fetchone()
        
        if result is None:
            print(f"âŒ é”™è¯¯: æ— æ³•æ‰¾åˆ°æ–‡æ¡£è®°å½•ï¼Œfilename='{q[1]}.pdf'")
            print(f"å¯èƒ½åŸå› : 1) æ–‡æ¡£æœªæ’å…¥æˆåŠŸ 2) æ–‡ä»¶åä¸åŒ¹é…")
            # æŸ¥è¯¢æœ€è¿‘æ’å…¥çš„æ–‡æ¡£
            cursor.execute(f"select top 5 TrustDocumentID, FileName from DV.TrustAssociatedDocument where Trustid={Trustid} order by TrustDocumentID desc")
            recent_docs = cursor.fetchall()
            print(f"è¯¥äº§å“æœ€è¿‘æ’å…¥çš„5ä¸ªæ–‡æ¡£:")
            for doc in recent_docs:
                print(f"  TrustDocumentID={doc[0]}, FileName={doc[1]}")
            return  # è·³è¿‡ProductsStateInformationæ’å…¥
        
        tdid = result[0]
        print(f"âœ“ æ‰¾åˆ°TrustDocumentID={tdid}")

        # æ£€æŸ¥ProductsStateInformationæ˜¯å¦å·²å­˜åœ¨
        check_task_sql = f"select 1 from TaskCollection.dbo.ProductsStateInformation where Trustid={Trustid} and TrustDocumentID={tdid}"
        cursor.execute(check_task_sql)
        if cursor.fetchone():
             print(f"Task already exists in ProductsStateInformation for TrustDocumentID={tdid}")

        else:
            # æ’å…¥ProductsStateInformation

            sql_i2 = f"insert into TaskCollection.dbo.ProductsStateInformation(Trustid,TrustDocumentID,FileType,StateType) values({Trustid}, {tdid}, {typy_q}, 9)"
            print(f"æ’å…¥ProductsStateInformation SQL: {sql_i2}")
            try:
                cursor.execute(sql_i2)
                print(f"âœ“ ProductsStateInformationæ’å…¥æˆåŠŸ: TrustDocumentID={tdid}")

            except Exception as e:
                print(f"âŒ ProductsStateInformationæ’å…¥å¤±è´¥: {e}")
                print(f"SQL: {sql_i2}")

                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­å¤„ç†

    conn.commit()
    # conn.close()
    print("single insert done")


# insert_db() å·²åˆ é™¤ - ä½¿ç”¨pymssqlçš„æ—§ç‰ˆæœ¬ï¼Œå·²è¢« insert_into_db() æ›¿ä»£

# checkDate() å·²åˆ é™¤ - è¿è¥æŠ¥å‘Šç›¸å…³åŠŸèƒ½ï¼Œä¸å±äºæ­¤è„šæœ¬


# insertDB() å·²åˆ é™¤ - è¿è¥æŠ¥å‘Šç›¸å…³åŠŸèƒ½ï¼Œä¸å±äºæ­¤è„šæœ¬


# upload_file() å·²åˆ é™¤ - æ—§ç‰ˆæ–‡ä»¶ä¸Šä¼ ï¼Œåªè¢«å·²åˆ é™¤çš„ upload_dir() ä½¿ç”¨


# crawl_pdf1() å·²åˆ é™¤ - è¿è¥æŠ¥å‘Šç›¸å…³åŠŸèƒ½ï¼Œä¸å±äºæ­¤è„šæœ¬

if __name__ == "__main__":

    # ä»FTPè¯»å–ä¸Šæ¬¡æ›´æ–°æ—¶é—´
    timestamp = read_ftp_file(ftp, UPDATE_LOG_PATH)
    print("ä¸Šæ¬¡æ›´æ–°çš„æ—¥æœŸä¸º " + timestamp)
    
    firstTimestamp, urls = get_html(timestamp)
    
    # save urls in abn_urls.txt line by line, overwriting the file
    with open('abn_urls.txt', 'w') as f:
        for url in urls:
            f.write(url + '\n')

    gmProds, smProds, qtProds = [], [], []
    gmEx = set()

    for idx, url in enumerate(urls, 1):
        print(f"\n{'='*80}")
        print(f"å¤„ç†è¿›åº¦: {idx}/{len(urls)}")
        print(f"{'='*80}")
        
        if len(url.split(':')) < 3:
            print(f"âš ï¸ è·³è¿‡æ ¼å¼é”™è¯¯çš„URL: {url}")
            continue

        url_path = url.split(':')[1]
        print(f"å¤„ç†URL: {url_path}")
        
        try:
            res = newProd(url_path)
            if res:
                if res[0] == 'å…¬å¼€å‘è¡Œ':
                    gmProds.append(res[1])
                    print(f"âœ“ å…¬å¼€å‘è¡Œäº§å“: {res[1]}")
                elif res[0] == 'å®šå‘å‘è¡Œ':
                    smProds.append(res[1])
                    print(f"âœ“ å®šå‘å‘è¡Œäº§å“: {res[1]}")
                else:
                    qtProds.append(res[1])
                    print(f"âœ“ å…¶ä»–ç±»å‹äº§å“: {res[1]}")
            else:
                print(f"âš ï¸ newProdè¿”å›Noneï¼Œè·³è¿‡æ­¤äº§å“")
        except Exception as e:
            print(f"âŒ å¤„ç†äº§å“æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            print(f"ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªäº§å“...")

    # æ›´æ–°æ—¶é—´åˆ°FTPï¼ˆåªæœ‰åœ¨æœ‰æ–°æ•°æ®æ—¶æ‰æ›´æ–°ï¼‰
    if urls:
        print(f"\n{'='*80}")
        print(f"æ›´æ–°æ—¶é—´æˆ³åˆ°FTP")
        print(f"{'='*80}")
        print(f"ä¸Šæ¬¡æ—¶é—´: {timestamp}")
        print(f"æœ¬æ¬¡æ—¶é—´: {firstTimestamp}")
        print(f"Writing latest date time {firstTimestamp} to {UPDATE_LOG_PATH}")
        firstTimestamp_str = firstTimestamp.strftime("%Y-%m-%d %H:%M:%S")

        with io.BytesIO(firstTimestamp_str.encode("utf-8")) as bio:
            ftp.storbinary(f"STOR {UPDATE_LOG_PATH}", bio)
        print("âœ“ æ—¶é—´æˆ³æ›´æ–°æˆåŠŸï¼")
    else:
        print("\nâš ï¸ æ²¡æœ‰æ–°æ•°æ®ï¼Œä¸æ›´æ–°æ—¶é—´æˆ³")


    print(f"æç¤º: å¦‚æœæœªè§¦è¾¾ï¼Œè¯·æ£€æŸ¥:");
    print(f"  1. è„šæœ¬çš„ 'START_DATE' å’Œ 'END_DATE' æˆ– 'MAX_PAGES' èŒƒå›´æ˜¯å¦è¦†ç›–äº†è¿™äº›äº§å“çš„å‘å¸ƒæ—¥æœŸ")
    print(f"  2. APIåˆ—è¡¨è¿”å›çš„æ ‡é¢˜æ˜¯å¦ä¸æä¾›çš„åç§°ä¸€è‡´")
    print(f"  3. æ˜¯å¦è¢«IPæ‹¦æˆªå¯¼è‡´æ•°æ®ä¸å…¨")
    print(f"{'='*80}\n")
    # ==================== DEBUG SUMMARY END ====================
    
    print(f"\n{'='*80}")
    print("âœ… è„šæœ¬æ‰§è¡Œå®Œæˆï¼")
    print(f"{'='*80}")
    print(f"å¤„ç†äº† {len(urls)} ä¸ªURL")
    print(f"å…¬å‹Ÿäº§å“: {len(gmProds)}, ç§å‹Ÿäº§å“: {len(smProds)}, å…¶ä»–: {len(qtProds)}")
    print(f"{'='*80}\n")
    

